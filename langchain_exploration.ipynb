{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1gYXReUjhzkbeieauMYCX7Zz1zGZTUpr0",
      "authorship_tag": "ABX9TyPdEVZL8thH85KrAcFGsHTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tykiww/LLM_Implementations/blob/main/langchain_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain Prompt Templating Practice\n",
        "\n",
        "Looking to create a structured prompt engine to stabilize the model persona.\n",
        "\n",
        "For this example, I am creating a simple summarizer.\n",
        "\n",
        "Inspo for this came from Sam Witteveen's Langchain Basics YT channel:\n",
        "https://www.youtube.com/watch?v=J_0qvRt4LNk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ\n",
        "\n",
        "I still need to get the hang of using google's flan-t5-xxl appropriately, but I will get there.\n",
        "\n",
        "That isn't the point of this exercise anyways.\n",
        "\n",
        "For replication, be sure to\n",
        "\n",
        "1) retrieve a huggingface API token and\n",
        "2) load the API token to a config.yaml file in your google drive."
      ],
      "metadata": {
        "id": "VTb8Ywc_Itaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geiKuAeTIr3U",
        "outputId": "0b930f81-221b-4057-8226-d786551c3c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip -q install langchain huggingface_hub torch pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "mzVJaJyIPNCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to retrieve credentials.\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "\n",
        "def google_drive():\n",
        "    \"\"\"Mount the google drive (colab only)\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def load_yaml(pathname):\n",
        "    \"\"\"retrieves Yaml file\"\"\"\n",
        "    with open(pathname, \"r\") as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    return config\n",
        "\n",
        "google_drive()\n",
        "config = load_yaml(\"drive/MyDrive/config.yaml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkNIIZb0P-dt",
        "outputId": "1187ed32-7fbe-47f4-fdd5-1a67157f1238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Session Info"
      ],
      "metadata": {
        "id": "8JL3nBmhPRHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what gpu (colab)?\n",
        "from torch import cuda\n",
        "\n",
        "\n",
        "def gpu_info():\n",
        "    \"\"\"Searches for supported GPU. Ecxepts all errors\"\"\"\n",
        "    try:\n",
        "        device = cuda.get_device_name()\n",
        "        n_gpus = cuda.device_count()\n",
        "        print(device + ',', n_gpus, 'NVIDIA GPU(s) found.')\n",
        "    except Exception as e:\n",
        "        print('Supported NVIDIA GPU not found or encountered an error:\\n', e)\n",
        "\n",
        "\n",
        "gpu_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0jemcvWTJID",
        "outputId": "9382820c-b01c-4317-dde2-a9f535b6d0af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supported NVIDIA GPU not found or encountered an error:\n",
            " Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling\n",
        "\n",
        "I tend to like keeping things modularized, but since it's kinda weird to do that in a notebook, everything is built as methods within a class\n",
        "\n",
        "#### <u>Model Object Setup</u>"
      ],
      "metadata": {
        "id": "-T10h3YPPVj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "D4U_Lf9XTL9O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLModelNavigator:\n",
        "    def __init__(self, oai_token,hf_token, modelname, model_kwargs):\n",
        "        self.hf_token = hf_token\n",
        "        self.oai_token = oai_token\n",
        "        self.modelname = modelname\n",
        "        self.model_kwargs = model_kwargs\n",
        "\n",
        "    def find_curly(self, text):\n",
        "        \"\"\"Helps to find all instances of input text using formatting\"\"\"\n",
        "        pattern = r'{(.*?)}'\n",
        "        matches = re.findall(pattern, text)\n",
        "        return matches\n",
        "\n",
        "    def set_tokens(self):\n",
        "        \"\"\"Sets the correct API token and is assigned upon model build\"\"\"\n",
        "        os.environ['HUGGINGFACEHUB_API_TOKEN'] = self.hf_token\n",
        "        os.environ['OPENAI_API_KEY'] = self.oai_token\n",
        "\n",
        "    def set_template(self, prompt_template):\n",
        "        \"\"\"Sets the prompt template based on the input text\"\"\"\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=self.find_curly(prompt_template),\n",
        "            template=prompt_template)\n",
        "        return prompt\n",
        "\n",
        "    def fewshot_template(self,kwargs):\n",
        "        \"\"\"Sets the prompt template based on an example prompt and text, input and separator\"\"\"\n",
        "        split = kwargs[\"text\"].split(kwargs[\"text_separator\"])\n",
        "\n",
        "        prompt = FewShotPromptTemplate(\n",
        "        examples=kwargs[\"examples\"],\n",
        "        example_prompt=kwargs[\"example_prompt\"],\n",
        "        prefix=split[0],\n",
        "        suffix=split[1],\n",
        "        input_variables=self.find_curly(kwargs[\"text\"]),\n",
        "        example_separator=kwargs[\"example_separator\"]\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Sets the token and accesses said model through huggingface\"\"\"\n",
        "        self.set_tokens()\n",
        "\n",
        "        model = HuggingFaceHub(\n",
        "            repo_id=self.modelname,\n",
        "            model_kwargs=self.model_kwargs) # ,task=\"text-generation\"\n",
        "\n",
        "        return model\n",
        "\n",
        "    def chain_model(self,prompt):\n",
        "        self.set_tokens()\n",
        "\n",
        "        model = HuggingFaceHub(\n",
        "            repo_id=self.modelname,\n",
        "            model_kwargs=self.model_kwargs) # ,task=\"text-generation\"\n",
        "        chain = LLMChain(llm=model, prompt=prompt)\n",
        "        return chain"
      ],
      "metadata": {
        "id": "tcCUN77kQZSO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = config['tokens']['openai']"
      ],
      "metadata": {
        "id": "UqLfAytdN48A"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Model Object Build</u>"
      ],
      "metadata": {
        "id": "bsTExrHGJa7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up model navigator\n",
        "navigator = LLModelNavigator(\n",
        "    oai_token = config['tokens']['openai'],\n",
        "    hf_token=config['tokens']['huggingface'],\n",
        "    modelname=\"google/flan-t5-xxl\", # Replace this field with similar task model repos..\n",
        "    model_kwargs={\"temperature\":.9,\n",
        "                  \"load_in_8bit\":True,\n",
        "                  \"device_map\": \"auto\",\n",
        "                  \"max_length\":512\n",
        "\n",
        "                  })\n"
      ],
      "metadata": {
        "id": "2FIRGnWBJZjb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>General Templated Approach</u>"
      ],
      "metadata": {
        "id": "-e0u5x9LJpPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt in the navigator\n",
        "prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Summarize the following text:\n",
        "    {answer}\n",
        "    \"\"\")\n",
        "\n",
        "# create chained model\n",
        "llm = navigator.chain_model(prompt)"
      ],
      "metadata": {
        "id": "iHDbnimLfUX8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not the greatest performance, but will do for now.\n",
        "\n",
        "llm.run(\"\"\"\n",
        " Peter and Elizabeth took a taxi to attend the night party in the city.\n",
        " While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
        " Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\n",
        " Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
        "        \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "osptLtEdwkH7",
        "outputId": "2ea41015-110b-49ac-fc04-ba3a69e3a586"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Peter stayed with Elizabeth for 3 days without leaving at the hospital.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>FewShot Templated Approach</u>\n",
        "\n",
        "Now this one is way more interesting.\n",
        "\n",
        "My guess is the structure gives more concrete guidance + flan-t5-xxl probably needs to be re-tuned for more verbose conversations."
      ],
      "metadata": {
        "id": "lrJG9fxd-Jdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"big\", \"antonym\": \"small\"}\n",
        "]\n",
        "\n",
        "# Next, create a generic example prompt with it\n",
        "example_prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Word: {word}\n",
        "    Antonym: {antonym}\\n\n",
        "    \"\"\")\n",
        "\n",
        "# After, create the actual prompt using the FewShot Template\n",
        "prompt_text = \"\"\"\n",
        "Give the antonym of every input\n",
        "\n",
        "Word: {input}\\nAntonym:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Bfg4hxYY-Wn1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a superhero version for fun.\n",
        "# Run this instead of the cell above for a different take.\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"superhero\": \"Superman\", \"villain\": \"Lex Luthor\"},\n",
        "    {\"superhero\": \"Batman\", \"villain\": \"Joker\"},\n",
        "    {\"superhero\": \"Spider-Man\", \"villain\": \"Green Goblin\"}\n",
        "]\n",
        "\n",
        "# Next, create a generic example prompt with it\n",
        "example_prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Superhero: {superhero}\n",
        "    Villain: {villain}\\n\n",
        "    \"\"\")\n",
        "\n",
        "# After, create the actual prompt using the FewShot Template\n",
        "prompt_text = \"\"\"\n",
        "Give the villain of every superhero input\n",
        "\n",
        "Superhero: {input}\\nVillain:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jQssFxorKXOu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the prompt in the navigator\n",
        "fewshot_prompt = navigator.fewshot_template(\n",
        "    kwargs={\n",
        "        \"examples\": examples,\n",
        "        \"example_prompt\": example_prompt,\n",
        "        \"text\": prompt_text,\n",
        "        \"text_separator\": \"\\n\\n\",\n",
        "        \"example_separator\": \"\\n\\n\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# create chained model\n",
        "llm = navigator.chain_model(fewshot_prompt)\n"
      ],
      "metadata": {
        "id": "GnAB8RKH-X_S"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not bad.\n",
        "llm.run(\"Peter Pan\").title()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WgXhe22R-Zbn",
        "outputId": "39121a1f-d096-4579-93a9-364bae4a3690"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hook'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Chaining Templates</u>\n",
        "\n",
        "Let's say that we want to connect with other APIs that interface with prompts.\n",
        "PALchain does a great job stringing math into programmatic formats.\n",
        "\n",
        "https://arxiv.org/pdf/2211.10435.pdf"
      ],
      "metadata": {
        "id": "Z0IoqWJ8BAhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import PALChain\n",
        "\n",
        "navigator = LLModelNavigator(\n",
        "    oai_token = '0',\n",
        "    hf_token=config['tokens']['huggingface'],\n",
        "    modelname=\"gpt2\", # Replace this field with similar task model repos..\n",
        "    model_kwargs={\"temperature\":.9,\n",
        "                  #\"load_in_8bit\":True,\n",
        "                  #\"device_map\": \"auto\",\n",
        "                  \"max_length\":500\n",
        "\n",
        "                  })\n",
        "\n",
        "llm = navigator.build_model()\n",
        "pal_chain = PALChain.from_math_prompt(llm, verbose=False)\n",
        "question = \"What is 1 + 45?\"\n",
        "pal_chain.run(question)\n",
        "\n",
        "# if this doesn't run, use an OpenAI authtoken instead of the class above.\n",
        "# There is a chance that the token length isn't supported with gpt2.\n",
        "# I just ran out of free credits and I'm a bit cheap right now."
      ],
      "metadata": {
        "id": "1fNoJibS1r7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}