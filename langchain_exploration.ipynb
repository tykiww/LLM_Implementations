{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1gYXReUjhzkbeieauMYCX7Zz1zGZTUpr0",
      "authorship_tag": "ABX9TyO4yUAesvuoajhZss4Mj2Qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tykiww/llm_implementations/blob/main/langchain_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain Prompt Templating Practice\n",
        "\n",
        "Looking to create a structured prompt engine to stabilize the model persona.\n",
        "\n",
        "For this example, I am creating a simple summarizer.\n",
        "\n",
        "Inspo for this came from Sam Witteveen's Langchain Basics YT channel:\n",
        "https://www.youtube.com/watch?v=J_0qvRt4LNk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ\n",
        "\n",
        "I still need to get the hang of using google's flan-t5-xxl appropriately, but I will get there.\n",
        "\n",
        "That isn't the point of this exercise anyways."
      ],
      "metadata": {
        "id": "VTb8Ywc_Itaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "geiKuAeTIr3U"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip -q install langchain huggingface_hub torch pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "mzVJaJyIPNCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to retrieve credentials.\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "\n",
        "def google_drive():\n",
        "    drive.mount('/content/drive',\n",
        "                force_remount=True)\n",
        "\n",
        "def load_yaml(pathname):\n",
        "    with open(pathname, \"r\") as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    return config\n",
        "\n",
        "google_drive()\n",
        "config = load_yaml(\"drive/MyDrive/config.yaml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkNIIZb0P-dt",
        "outputId": "fb5a3dca-c09c-401e-f554-bddcafd00b23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Session Info"
      ],
      "metadata": {
        "id": "8JL3nBmhPRHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what gpu (colab)?\n",
        "from torch import cuda\n",
        "\n",
        "def gpu_info():\n",
        "    device = cuda.get_device_name()\n",
        "    n_gpus = cuda.device_count()\n",
        "    print(device+',', n_gpus)\n",
        "\n",
        "gpu_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0jemcvWTJID",
        "outputId": "22e06fd2-21d5-4cc5-aaba-ebbbc15af167"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling\n",
        "\n",
        "I tend to like keeping things as a cohesive unit, so everything is built as methods within a class\n",
        "\n",
        "#### <u>Model Object Setup</u>"
      ],
      "metadata": {
        "id": "-T10h3YPPVj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "D4U_Lf9XTL9O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLModelNavigator:\n",
        "    def __init__(self, token, modelname, model_kwargs):\n",
        "        self.token = token\n",
        "        self.modelname = modelname\n",
        "        self.model_kwargs = model_kwargs\n",
        "\n",
        "    def find_curly(self, text):\n",
        "        pattern = r'{(.*?)}'\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def find_curly(self, text):\n",
        "        pattern = r'{(.*?)}'\n",
        "        matches = re.findall(pattern, text)\n",
        "        return matches\n",
        "\n",
        "    def set_tokens(self):\n",
        "        os.environ['HUGGINGFACEHUB_API_TOKEN'] = self.token\n",
        "\n",
        "    def set_template(self, prompt_template):\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=self.find_curly(prompt_template),\n",
        "            template=prompt_template)\n",
        "        return prompt\n",
        "\n",
        "    def fewshot_template(self,kwargs):\n",
        "        split = kwargs[\"text\"].split(kwargs[\"text_separator\"])\n",
        "\n",
        "        prompt = FewShotPromptTemplate(\n",
        "        examples=kwargs[\"examples\"],\n",
        "        example_prompt=kwargs[\"example_prompt\"],\n",
        "        prefix=split[0],\n",
        "        suffix=split[1],\n",
        "        input_variables=self.find_curly(kwargs[\"text\"]),\n",
        "        example_separator=kwargs[\"example_separator\"]\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def build_model(self,prompt):\n",
        "        self.set_tokens()\n",
        "\n",
        "        model = HuggingFaceHub(\n",
        "            repo_id=self.modelname,\n",
        "            model_kwargs=self.model_kwargs)\n",
        "        chain = LLMChain(llm=model, prompt=prompt)\n",
        "\n",
        "        return chain"
      ],
      "metadata": {
        "id": "tcCUN77kQZSO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Model Object Build</u>"
      ],
      "metadata": {
        "id": "bsTExrHGJa7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up model navigator\n",
        "navigator = LLModelNavigator(\n",
        "    token=config['tokens']['huggingface'],\n",
        "    modelname=\"google/flan-t5-xxl\", # Replace this field with similar sized model repos..\n",
        "    model_kwargs={\"temperature\":.1,\n",
        "                  \"max_length\":512,\n",
        "                  \"load_in_8bit\":True,\n",
        "                  \"device_map\": \"auto\"})\n"
      ],
      "metadata": {
        "id": "2FIRGnWBJZjb"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>General Templated Approach</u>"
      ],
      "metadata": {
        "id": "-e0u5x9LJpPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt in the navigator\n",
        "prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Summarize the following text:\n",
        "    {answer}\n",
        "    \"\"\")\n",
        "\n",
        "# create chained model\n",
        "llm = navigator.build_model(prompt)"
      ],
      "metadata": {
        "id": "iHDbnimLfUX8"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not the greatest performance, but will do for now.\n",
        "\n",
        "llm.run(\"\"\"\n",
        " Peter and Elizabeth took a taxi to attend the night party in the city.\n",
        " While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
        " Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\n",
        " Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
        "        \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "osptLtEdwkH7",
        "outputId": "3244e856-62ac-4864-c24e-2f0bff11b28f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Peter stayed with Elizabeth at the hospital for 3 days.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>FewShot Templated Approach</u>\n",
        "\n",
        "Now this one is way more interesting.\n",
        "\n",
        "My guess is the structure gives more concrete guidance"
      ],
      "metadata": {
        "id": "lrJG9fxd-Jdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"big\", \"antonym\": \"small\"}\n",
        "]\n",
        "\n",
        "# Next, create a generic example prompt with it\n",
        "example_prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Word: {word}\n",
        "    Antonym: {antonym}\\n\n",
        "    \"\"\")\n",
        "\n",
        "# After, create the actual prompt using the FewShot Template\n",
        "prompt_text = \"\"\"\n",
        "Give the antonym of every input\n",
        "\n",
        "Word: {input}\\nAntonym:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Bfg4hxYY-Wn1"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a superhero version for fun.\n",
        "# Run this instead of the cell above for a different take.\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"superhero\": \"Superman\", \"villain\": \"Lex Luthor\"},\n",
        "    {\"superhero\": \"Batman\", \"villain\": \"Joker\"},\n",
        "    {\"superhero\": \"Spider-Man\", \"villain\": \"Green Goblin\"}\n",
        "]\n",
        "\n",
        "# Next, create a generic example prompt with it\n",
        "example_prompt = navigator.set_template(prompt_template=\n",
        "    \"\"\"\n",
        "    Superhero: {superhero}\n",
        "    Villain: {villain}\\n\n",
        "    \"\"\")\n",
        "\n",
        "# After, create the actual prompt using the FewShot Template\n",
        "prompt_text = \"\"\"\n",
        "Give the villain of every superhero input\n",
        "\n",
        "Superhero: {input}\\nVillain:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jQssFxorKXOu"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the prompt in the navigator\n",
        "fewshot_prompt = navigator.fewshot_template(\n",
        "    kwargs={\n",
        "        \"examples\": examples,\n",
        "        \"example_prompt\": example_prompt,\n",
        "        \"text\": prompt_text,\n",
        "        \"text_separator\": \"\\n\\n\",\n",
        "        \"example_separator\": \"\\n\\n\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# create chained model\n",
        "llm = navigator.build_model(fewshot_prompt)\n"
      ],
      "metadata": {
        "id": "GnAB8RKH-X_S"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not bad.\n",
        "llm.run(\"The Flash\").title()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WgXhe22R-Zbn",
        "outputId": "c5c44db5-9321-4a91-ec85-88aeaaff0141"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zoom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    }
  ]
}